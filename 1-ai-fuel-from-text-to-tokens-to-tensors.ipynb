{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mds34Dbh0ty6"
      },
      "source": [
        "# AI's Fuel: From Text to Tokens to Tensors\n",
        "\n",
        "This notebook turns a piece of text into something a computer can use: a tensor. We begin with a sentence, transform it to tokens, and end with a tensor made of one-hot vectors. Each step guides you through this process."
      ],
      "id": "mds34Dbh0ty6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmqCSmYZ0ty-"
      },
      "source": [
        "## Step 1 — Provide the text\n",
        "Edit the cell below to set the text to convert to tokens. Keep the variable name exactly as `TEXT`."
      ],
      "id": "NmqCSmYZ0ty-"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uicIW6l20tzA"
      },
      "outputs": [],
      "source": [
        "TEXT = \"Tokens are text that become numbers that were text\""
      ],
      "id": "uicIW6l20tzA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E1t7Aiz0tzB"
      },
      "source": [
        "## Step 2 — Get the tokens\n",
        "For simplicity, we ignore punctuation and tokenize based on words. We normalize the text by making it lowercase. Then we split on spaces to obtain the words in the sentence."
      ],
      "id": "_E1t7Aiz0tzB"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFiVoyQb0tzC",
        "outputId": "93fbe5d7-deeb-484d-fec4-4dbf63985097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['tokens', 'are', 'text', 'that', 'become', 'numbers', 'that', 'were', 'text']\n"
          ]
        }
      ],
      "source": [
        "clean = TEXT.lower()\n",
        "tokens = clean.split()\n",
        "print(\"Tokens:\", tokens)"
      ],
      "id": "vFiVoyQb0tzC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9JO7sK50tzC"
      },
      "source": [
        "## Step 3 — Collect the vocabulary\n",
        "From the words we saw, we gather the unique ones and find the vocabulary size."
      ],
      "id": "S9JO7sK50tzC"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSI_ehYH0tzD",
        "outputId": "309902d5-5093-4f9d-d278-a394dd270bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'become', 'that', 'numbers', 'tokens', 'were', 'text', 'are'}\n",
            "Vocabulary size: 7\n"
          ]
        }
      ],
      "source": [
        "vocab = set(tokens)\n",
        "V = len(vocab)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Vocabulary size:\", V)"
      ],
      "id": "gSI_ehYH0tzD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5vcO0b_0tzD"
      },
      "source": [
        "## Step 4 — Get the indices\n",
        "Each vocabulary word receives a unique number. These numbers will be used to identify the words in the vocabulary. Note that repeated words disappear."
      ],
      "id": "Y5vcO0b_0tzD"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIVwYJpW0tzE",
        "outputId": "9dc3fbf5-3933-4eb7-eb94-6f0b9a4d56fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary with index: {'become': 0, 'that': 1, 'numbers': 2, 'tokens': 3, 'were': 4, 'text': 5, 'are': 6}\n",
            "Index with vocabulary: {0: 'become', 1: 'that', 2: 'numbers', 3: 'tokens', 4: 'were', 5: 'text', 6: 'are'}\n"
          ]
        }
      ],
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "print(\"Vocabulary with index:\", word2idx)\n",
        "print(\"Index with vocabulary:\", idx2word)"
      ],
      "id": "TIVwYJpW0tzE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIftCZnk0tzF"
      },
      "source": [
        "## Step 5 — Define one-hot vectors\n",
        "A one-hot vector is a vector that has zeros everywhere and a single 1 at the index of the word. The length of the vector equals the vocabulary size."
      ],
      "id": "dIftCZnk0tzF"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0IItpEr0tzF",
        "outputId": "cb0bc3e3-1c58-420b-8f53-892370a60f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example one-hot for word 'that' (index 1): [0 1 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def one_hot(idx, vocab_size):\n",
        "    vec = np.zeros(vocab_size, dtype=int)\n",
        "    vec[idx] = 1\n",
        "    return vec\n",
        "\n",
        "# Play with the example_index\n",
        "example_index = 1\n",
        "example_word = idx2word[example_index]\n",
        "print(f\"Example one-hot for word '{example_word}' (index {example_index}):\", one_hot(example_index, V))"
      ],
      "id": "h0IItpEr0tzF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDS3oWKR0tzG"
      },
      "source": [
        "## Step 6 — Translate the sentence into indices\n",
        "We replace each token in the sentence with its index from the vocabulary."
      ],
      "id": "JDS3oWKR0tzG"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPLuqVHo0tzG",
        "outputId": "a8b50b6a-24bf-48fe-9103-90418402fddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: Tokens are text that become numbers that were text\n",
            "Vocabulary with index: {'become': 0, 'that': 1, 'numbers': 2, 'tokens': 3, 'were': 4, 'text': 5, 'are': 6}\n",
            "Token indices based on word2idx: [3, 6, 5, 1, 0, 2, 1, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "token_indices = [word2idx[t] for t in tokens]\n",
        "print(\"Original sentence:\", TEXT)\n",
        "print(\"Vocabulary with index:\", word2idx)\n",
        "print(\"Token indices based on word2idx:\", token_indices)"
      ],
      "id": "HPLuqVHo0tzG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTgK0xex0tzG"
      },
      "source": [
        "## Step 7 — Get the tensor\n",
        "For every word index we create a one-hot vector, then stack all those vectors into an array. This results in our tensor, where each row corresponds to a token (in sentence order) and each column to a vocabulary word."
      ],
      "id": "MTgK0xex0tzG"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkAzxKHV0tzG",
        "outputId": "c4a31f5b-b684-4765-93f2-7f9ef4aa4ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: Tokens are text that become numbers that were text\n",
            "Vocabulary with index: {'become': 0, 'that': 1, 'numbers': 2, 'tokens': 3, 'were': 4, 'text': 5, 'are': 6}\n",
            "Token indices: [3, 6, 5, 1, 0, 2, 1, 4, 5]\n",
            "Tensor shape: (9, 7)\n",
            "[[0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "one_hots = [one_hot(i, V) for i in token_indices]\n",
        "tensor = np.stack(one_hots, axis=0)\n",
        "print(\"Original sentence:\", TEXT)\n",
        "print(\"Vocabulary with index:\", word2idx)\n",
        "print(\"Token indices:\", token_indices)\n",
        "print(\"Tensor shape:\", tensor.shape)\n",
        "print(tensor)"
      ],
      "id": "YkAzxKHV0tzG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Like this notebook?\n",
        "[<img src=\"https://raw.githubusercontent.com/primer/octicons/main/icons/mark-github-16.svg\" alt=\"GitHub\" width=\"18\" style=\"vertical-align: text-bottom;\"> Data & AI with Forbes](https://github.com/bforbesc/Data-AI-with-Forbes)\n",
        "\n",
        "If you enjoyed this walkthrough, check out my repo for more technical data-and-AI topics simply and clearly explained."
      ],
      "metadata": {
        "id": "VPvuBGlmKy-L"
      },
      "id": "VPvuBGlmKy-L"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}